{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "\n",
    "## Project 5: Capstone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data read successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load training data\n",
    "train_data = pd.read_csv(\"final_train_data.csv\")\n",
    "target_train = train_data['CFA']\n",
    "feature_train = train_data.drop(['CFA','ID'], axis = 1)\n",
    "print \"Training data read successfully!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating data read successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load validating data\n",
    "validate_data = pd.read_csv(\"validating_data.csv\")\n",
    "target_validate = validate_data['CFA']\n",
    "feature_validate = validate_data.drop(['CFA','ID'], axis = 1)\n",
    "print \"validating data read successfully!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing data read successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load testing data\n",
    "test_data = pd.read_csv(\"test_data.csv\")\n",
    "target_test = test_data['CFA']\n",
    "feature_test = test_data.drop(['CFA','ID'], axis = 1)\n",
    "print \"testing data read successfully!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic information of training and testing dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns for training data set: (201606, 112)\n",
      "Number of rows and columns for validating data set: (574, 112)\n",
      "Number of rows and columns for testing data set: (674, 112)\n"
     ]
    }
   ],
   "source": [
    "print 'Number of rows and columns for training data set:', feature_train.shape\n",
    "print 'Number of rows and columns for validating data set:', feature_validate.shape\n",
    "print 'Number of rows and columns for testing data set:', feature_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Training and Evaluating Models\n",
    "\n",
    "\n",
    "**The following supervised learning models are selected to use.**\n",
    "- Gaussian Naive Bayes (GaussianNB)\n",
    "- Decision Trees\n",
    "- Logistic Regression\n",
    "- Ensemble Methods (AdaBoost)\n",
    "\n",
    "**Evaluation merits:  F1 score. F1 = 2 * (precision * recall) / (precision + recall)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import algorithm from sklearn\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier as DT\n",
    "from sklearn.ensemble import AdaBoostClassifier as AB\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 GaussianNB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for test set: 0.8213.\n",
      "Process in 1.8210 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "clf = GaussianNB()\n",
    "clf.fit(feature_train,target_train)\n",
    "pred = clf.predict(feature_validate)\n",
    "f1 = f1_score(target_validate, pred)\n",
    "end = time()\n",
    "print \"F1 score for test set: {:.4f}.\".format(f1)\n",
    "print \"Process in {:.4f} seconds.\".format(end - start)\n",
    "# full dataset with the score of 0.8285"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for test set: 0.8402.\n",
      "Process in 9.9120 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "clf = DT()\n",
    "clf.fit(feature_train,target_train)\n",
    "pred = clf.predict(feature_validate)\n",
    "f1 = f1_score(target_validate, pred)\n",
    "end = time()\n",
    "print \"F1 score for test set: {:.4f}.\".format(f1)\n",
    "print \"Process in {:.4f} seconds.\".format(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 AdaBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for test set: 0.8449.\n",
      "Process in 32.0410 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "clf = AB()\n",
    "clf.fit(feature_train,target_train)\n",
    "pred = clf.predict(feature_validate)\n",
    "f1 = f1_score(target_validate, pred)\n",
    "end = time()\n",
    "print \"F1 score for test set: {:.4f}.\".format(f1)\n",
    "print \"Process in {:.4f} seconds.\".format(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for test set: 0.8425.\n",
      "Process in 3.4310 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "clf = LR()\n",
    "clf.fit(feature_train,target_train)\n",
    "pred = clf.predict(feature_validate)\n",
    "f1 = f1_score(target_validate, pred)\n",
    "end = time()\n",
    "print \"F1 score for test set: {:.4f}.\".format(f1)\n",
    "print \"Process in {:.4f} seconds.\".format(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Choosing the Best Model and Model Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Best Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=100, min_samples_leaf=1,\n",
      "            min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='random')\n",
      "F1 score for test set: 0.8514.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Initialize the classifier\n",
    "clf = DT()\n",
    "\n",
    "# Create the parameters list you wish to tune\n",
    "parameters = {'criterion': ('gini','entropy'),\n",
    "              'splitter':('best','random'),\n",
    "              'min_samples_split':[2,10,20],\n",
    "                'max_leaf_nodes':[5,30,100]}\n",
    "\n",
    "\n",
    "\n",
    "# Make an f1 scoring function using 'make_scorer' \n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "# Perform grid search on the classifier using the f1_scorer as the scoring method\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring=f1_scorer)\n",
    "\n",
    "#  Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj = grid_obj.fit(feature_train,target_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "print clf\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "pred = clf.predict(feature_validate)\n",
    "f1 = f1_score(target_validate, pred)\n",
    "print \"F1 score for test set: {:.4f}.\".format(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 AdaBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=100, random_state=None)\n",
      "F1 score for test set: 0.8478.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the classifier\n",
    "clf = AB()\n",
    "\n",
    "# Create the parameters list you wish to tune\n",
    "parameters = {\n",
    "              \"n_estimators\": [1, 50, 100]\n",
    "             }\n",
    "\n",
    "\n",
    "\n",
    "# Make an f1 scoring function using 'make_scorer' \n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "# Perform grid search on the classifier using the f1_scorer as the scoring method\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring=f1_scorer)\n",
    "\n",
    "#  Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj = grid_obj.fit(feature_train,target_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "print clf\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "pred = clf.predict(feature_validate)\n",
    "f1 = f1_score(target_validate, pred)\n",
    "print \"F1 score for test set: {:.4f}.\".format(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "F1 score for test set: 0.8420.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the classifier\n",
    "clf = LR()\n",
    "\n",
    "# Create the parameters list you wish to tune\n",
    "parameters = {'penalty':('l1','l2'),\n",
    "              'C':[ 0.01,  0.1, 1.0, 10, 20]}\n",
    "\n",
    "\n",
    "\n",
    "# Make an f1 scoring function using 'make_scorer' \n",
    "f1_scorer = make_scorer(f1_score)\n",
    "\n",
    "# Perform grid search on the classifier using the f1_scorer as the scoring method\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring=f1_scorer)\n",
    "\n",
    "#  Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj = grid_obj.fit(feature_train,target_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "print clf\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "pred = clf.predict(feature_validate)\n",
    "f1 = f1_score(target_validate, pred)\n",
    "print \"F1 score for test set: {:.4f}.\".format(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final F<sub>1</sub> Score and Robustness of the final classifier\n",
    "\n",
    "**The best f1 score is 0.8514 with DecisionTreeClassifier** (class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=100, min_samples_leaf=1,\n",
    "            min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
    "            presort=False, random_state=None, splitter='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for training set: 0.8765.\n",
      "F1 score for testing set: 0.8630.\n"
     ]
    }
   ],
   "source": [
    "clf = DT(class_weight=None, criterion='gini', max_depth=None, max_features=None, max_leaf_nodes=100, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='random')\n",
    "clf.fit(feature_train,target_train)\n",
    "\n",
    "# calculate f1 for training\n",
    "pred_train = clf.predict(feature_train)\n",
    "f1_train = f1_score(target_train, pred_train)\n",
    "\n",
    "\n",
    "#calculate f1 for testing\n",
    "pred_test = clf.predict(feature_test)\n",
    "f1_test = f1_score(target_test, pred_test)\n",
    "\n",
    "print \"F1 score for training set: {:.4f}.\".format(f1_train)\n",
    "print \"F1 score for testing set: {:.4f}.\".format(f1_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comfusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 41 131]\n",
      " [ 40 462]]\n",
      "\n",
      "\n",
      "[[ 37 135]\n",
      " [ 18 484]]\n",
      "\n",
      "\n",
      "[[ 32 140]\n",
      " [  9 493]]\n",
      "\n",
      "\n",
      "[[ 19 153]\n",
      " [ 10 492]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "clf_A = GaussianNB()\n",
    "\n",
    "clf_B = DT(class_weight=None, criterion='gini', max_depth=None, max_features=None, max_leaf_nodes=100, min_samples_leaf=1, min_samples_split=10, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='random')\n",
    "\n",
    "clf_C = AB(algorithm='SAMME.R', base_estimator=None,\n",
    "          learning_rate=1.0, n_estimators=100, random_state=None)\n",
    "\n",
    "clf_D = LR(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
    "          verbose=0, warm_start=False)\n",
    "\n",
    "for clf in [clf_A, clf_B, clf_C, clf_D]:\n",
    "    clf.fit(feature_train,target_train)\n",
    "    pred = clf.predict(feature_test)\n",
    "    matrix = confusion_matrix(target_test, pred)\n",
    "    print matrix\n",
    "    print '\\n'\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
